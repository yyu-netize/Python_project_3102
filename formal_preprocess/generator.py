from openai import OpenAI
import torch
from search_bgem3 import UltimateRAG

# --- CONFIGURATION ---
SILICONFLOW_API_KEY = "sk-bpdybiceumehnyfudsglnizvhqssgpsjpusvienlfgchijdl"  # SiliconFlow API å¯†é’¥
MODEL_NAME = "Qwen/Qwen3-8B"      

# Initialize the Client pointing to SiliconFlow
client = OpenAI(
    base_url="https://api.siliconflow.cn/v1/",  
    api_key=SILICONFLOW_API_KEY,
)

print(f"\nSUCCESS: Client initialized using model: {MODEL_NAME}")

class UltimateRAGWithGenerator:
    def __init__(self):
        # æ¨¡å‹é…ç½®
        self.client = client
        self.model_name = MODEL_NAME
        # åˆå§‹åŒ– UltimateRAG å®ä¾‹
        self.rag = UltimateRAG()

    def generate_answer(self, query, candidates, prompt_mode, message_mode): #prompt_modeå¯é€‰ï¼š1.vanillaï¼Œ2.instructionï¼Œ  message_modeå¯é€‰ï¼š1.with_systemï¼Œ2.no_system
        """
        ä½¿ç”¨ Qwen3-8B ç”Ÿæˆæœ€ç»ˆçš„å›ç­”
        """
        # åˆå¹¶å€™é€‰æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡
        unique_candidates = {cand['text'] for cand in candidates}
        context = " ".join(unique_candidates)

        system_msg = ""
        user_msg = ""
        prompt = ""
        if message_mode == "with_system":
            if prompt_mode == "vanilla":
                system_msg = "You are a helpful AI assistant."
                user_msg = f"""
Question: {query}
---
Context:
{context}
---
Answer the question using the context. If the answer is not mentioned, say you don't know.
"""
            elif prompt_mode == "instruction":
                system_msg = (
                    "You are a retrieval-augmented QA assistant for Plants vs Zombies.\n"
                    "You must answer ONLY based on the provided context.\n"
                    "Never hallucinate facts that do not appear in the context."
                )
                user_msg = f"""
Question:
{query}
---
Context:
{context}
---
Now please provide accurate, concise, and complete answers using ONLY the provided context. 
Do not hallucinate external details. Avoid redundancy.
Write complete sentences.
If the answer is not mentioned, say you don't know.
"""
            try:
                response = client.chat.completions.create(
                    model=self.model_name,
                
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": user_msg},
                    ],
                    stop=None,
                    temperature=0.7,
                    top_p=1.0,
                    n=1,
                )
                answer = response.choices[0].message.content.strip()  # è·å–ç”Ÿæˆçš„å›ç­”
                return answer
            
            except Exception as e:
                print(f"Error during generation: {e}")
                return "Sorry, I couldn't generate an answer at the moment."

        elif message_mode == "no_system":
            if prompt_mode == "vanilla":
                prompt = f"""
Use the following context to answer the question. 
If the answer is not in the context, respond with "I don't know".
---
Question:
{query}
---
Context:
{context}
---
Now please provide an answer.
"""
            elif prompt_mode == "instruction":
                prompt = f"""
You are a Plants vs. Zombies domain expert.
Your job is to provide accurate, concise, and complete answers using ONLY the provided context. 
Do not hallucinate external details. Avoid redundancy.
Write complete sentences.
If the answer is not in the context, respond with "I don't know".
---
Question:
{query}
---
Context:
{context}
---
Now provide an answer.
"""
            try:
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": prompt},
                    ],
                    stop=None,
                    temperature=0.7,
                    top_p=1.0,
                    n=1,
                )
                answer = response.choices[0].message.content.strip()  # è·å–ç”Ÿæˆçš„å›ç­”
                return answer
            
            except Exception as e:
                print(f"Error during generation: {e}")
                return "Sorry, I couldn't generate an answer at the moment."

    def search(self, query, retrieve_mode, prompt_mode, message_mode):
        """
        è¿›è¡Œæ£€ç´¢å¹¶ç”Ÿæˆæœ€ç»ˆçš„å›ç­”
        """
        print(f"\nğŸ” Query: {query}")
        candidate = []

        # 1. æ··åˆå¬å› (Recall) - è·å–å€™é€‰æ–‡æœ¬ (ä¾‹å¦‚ 30 ä¸ª)
        if retrieve_mode == "hyde":
            hyde_doc = self.rag.hyde_generate_doc(query)
            print(f"\nğŸ“„ HyDE Generated Document:\n{hyde_doc}\n")
            # dense embedding from HyDE doc
            candidates = self.rag.retrieve_dense(hyde_doc, top_k=30)
            print(f"   - HyDE Dense Retrieval æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ")
        if retrieve_mode == "hybrid":
            candidates = self.rag.retrieve_hybrid(query, top_k=30)
            print(f"   - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (Vector + BM25)")
        elif retrieve_mode == "dense":
            candidates = self.rag.retrieve_dense(query, top_k=30)
            print(f"   - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (Vector)")
        elif retrieve_mode == "bm25":
            candidates = self.rag.retrieve_bm25(query, top_k=30)
            print(f"   - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (BM25)")
        

        # 2. é‡æ’åº (Rerank) - æç‚¼ Top 5
        final_results = self.rag.rerank(query, candidates, top_n=5)

        # 3. ä½¿ç”¨ Qwen3-8B ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        answer = self.generate_answer(query, final_results, prompt_mode, message_mode)#prompt_modeå¯é€‰ï¼š1.vanillaï¼Œ2.instructionï¼Œ  message_modeå¯é€‰ï¼š1.with_systemï¼Œ2.no_system

        # 4. å±•ç¤ºç”Ÿæˆçš„ç­”æ¡ˆ
        print(f"Answer:\n {answer}")
        return answer


if __name__ == "__main__":
    # åˆå§‹åŒ– RAG å¼•æ“
    rag_with_generator = UltimateRAGWithGenerator()
    
    # æµ‹è¯•æŸ¥è¯¢
    rag_with_generator.search("Which plant can slow down zombies?", retrieve_mode="hybrid", prompt_mode="instruction", message_mode="with_system")
    rag_with_generator.search("What is the sun cost of Peashooter?", retrieve_mode="dense", prompt_mode="instruction", message_mode="with_system")
    rag_with_generator.search("Difference between Cherry Bomb and Jalapeno", retrieve_mode="bm25", prompt_mode="instruction", message_mode="with_system")

    # æ‰‹åŠ¨è¾“å…¥æŸ¥è¯¢
    # æ‰‹åŠ¨è¾“å…¥æŸ¥è¯¢ï¼ˆæ”¯æŒå‚æ•°é…ç½®ï¼‰
while True:
    print("\n=== RAG æŸ¥è¯¢ç³»ç»Ÿ ===")
    print("è¾“å…¥æ ¼å¼ç¤ºä¾‹: ä½ çš„é—®é¢˜ | retrieve_mode | prompt_mode | message_mode")
    print("å‚æ•°è¯´æ˜:")
    print("- retrieve_mode: hybrid / dense / sparse (é»˜è®¤: hybrid)")
    print("- prompt_mode: vanilla / instruction (é»˜è®¤: instruction)")
    print("- message_mode: with_system / no_system (é»˜è®¤: with_system)")
    print("ç›´æ¥è¾“å…¥ q é€€å‡ºï¼Œåªè¾“å…¥é—®é¢˜åˆ™ä½¿ç”¨é»˜è®¤å‚æ•°")
    
    user_input = input("\nè¯·è¾“å…¥æŸ¥è¯¢å†…å®¹: ")
    
    # é€€å‡ºæ¡ä»¶
    if user_input.lower() == 'q':
        break
    
    # è§£æè¾“å…¥å†…å®¹
    parts = [part.strip() for part in user_input.split('|')]
    query = parts[0] if parts[0] else None
    
    # è®¾ç½®é»˜è®¤å‚æ•°
    retrieve_mode = "hybrid"
    prompt_mode = "instruction"
    message_mode = "with_system"
    
    # æ›´æ–°å‚æ•°ï¼ˆå¦‚æœç”¨æˆ·æä¾›äº†ï¼‰
    if len(parts) >= 2 and parts[1]:
        retrieve_mode = parts[1]
    if len(parts) >= 3 and parts[2]:
        prompt_mode = parts[2]
    if len(parts) >= 4 and parts[3]:
        message_mode = parts[3]
    
    # éªŒè¯å‚æ•°æœ‰æ•ˆæ€§
    valid_retrieve_modes = ["hybrid", "dense", "sparse"]
    valid_prompt_modes = ["vanilla", "instruction"]
    valid_message_modes = ["with_system", "no_system"]
    
    if retrieve_mode not in valid_retrieve_modes:
        print(f"æ— æ•ˆçš„ retrieve_mode: {retrieve_mode}ï¼Œä½¿ç”¨é»˜è®¤å€¼ hybrid")
        retrieve_mode = "hybrid"
    
    if prompt_mode not in valid_prompt_modes:
        print(f"æ— æ•ˆçš„ prompt_mode: {prompt_mode}ï¼Œä½¿ç”¨é»˜è®¤å€¼ instruction")
        prompt_mode = "instruction"
    
    if message_mode not in valid_message_modes:
        print(f"æ— æ•ˆçš„ message_mode: {message_mode}ï¼Œä½¿ç”¨é»˜è®¤å€¼ with_system")
        message_mode = "with_system"
    
    # æ‰§è¡ŒæŸ¥è¯¢
    if query:
        print(f"\næ‰§è¡ŒæŸ¥è¯¢ - retrieve_mode: {retrieve_mode}, prompt_mode: {prompt_mode}, message_mode: {message_mode}")
        rag_with_generator.search(
            query, 
            retrieve_mode=retrieve_mode, 
            prompt_mode=prompt_mode, 
            message_mode=message_mode
        )
    else:
        print("æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©ºï¼")