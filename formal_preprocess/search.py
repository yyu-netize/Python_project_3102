import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import chromadb
from rank_bm25 import BM25Okapi
import pickle
import json
import os

# --- é…ç½®  ---
DB_DIR = "./chroma_db_m3"
BM25_PATH = "./bm25_m3.pkl"

# æ¨¡å‹å®šä¹‰
EMBEDDING_MODEL_NAME = "Alibaba-NLP/gte-Qwen2-7B-instruct"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-v2-m3" # å¼ºåŠ›é‡æ’æ¨¡å‹

# --- æ˜¾å¡é…ç½® ---
DEVICE = "cuda"

class UltimateRAG:
    def __init__(self):
        print("âš™ï¸  æ­£åœ¨åˆå§‹åŒ– RAG å¼•æ“ (åŠ è½½æ¨¡å‹éœ€å ç”¨çº¦ 18GB æ˜¾å­˜)...")
        
        # 1. åŠ è½½ Embedding æ¨¡å‹ (7B)
        print(f"   [1/4] åŠ è½½ Embedding æ¨¡å‹: {EMBEDDING_MODEL_NAME} ...")
        self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME, trust_remote_code=True)
        self.embed_model = AutoModel.from_pretrained(
            EMBEDDING_MODEL_NAME, 
            trust_remote_code=True, 
            torch_dtype=torch.float16 # FP16 åŠ é€Ÿ
        ).to(DEVICE)
        self.embed_model.eval()

        # 2. åŠ è½½ Reranker æ¨¡å‹ (Cross-Encoder)
        print(f"   [2/4] åŠ è½½ Reranker æ¨¡å‹: {RERANKER_MODEL_NAME} ...")
        self.rerank_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)
        self.rerank_model = AutoModelForSequenceClassification.from_pretrained(
            RERANKER_MODEL_NAME,
            torch_dtype=torch.float16
        ).to(DEVICE)
        self.rerank_model.eval()

        # 3. è¿æ¥ ChromaDB
        print("   [3/4] è¿æ¥å‘é‡æ•°æ®åº“...")
        self.client = chromadb.PersistentClient(path=DB_DIR)
        self.collection = self.client.get_collection("pvz_knowledge_m3")

        # 4. åŠ è½½ BM25
        print("   [4/4] åŠ è½½ BM25 ç´¢å¼•...")
        with open(BM25_PATH, 'rb') as f:
            data = pickle.load(f)
            self.bm25 = data['bm25']
            self.bm25_chunks = data['chunks'] # BM25 éœ€è¦åŸå§‹ chunks åˆ—è¡¨æ¥å®šä½ç»“æœ

        print("âœ… RAG å¼•æ“å°±ç»ª! ç­‰å¾…æŒ‡ä»¤...\n")

    def last_token_pool(self, last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
        """ä¸ build è„šæœ¬ä¿æŒä¸€è‡´çš„ Pooling ç­–ç•¥"""
        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
        if left_padding:
            return last_hidden_states[:, -1]
        else:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = last_hidden_states.shape[0]
            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

    def get_query_embedding(self, query):
        """
        ä¸º gte-Qwen2 æ·»åŠ æ£€ç´¢æŒ‡ä»¤
        """
        # æŒ‡ä»¤ï¼šå®šä¹‰ä»»åŠ¡æ€§è´¨
        task_instruction = "Retrieve detailed attributes, stats, and strategies for Plants vs. Zombies game entities."
        # æ ¼å¼ï¼šInstruction + \n + Query
        prompt = f"Instruction: {task_instruction}\nQuery: {query}"
        
        inputs = self.tokenizer(
            [prompt], 
            max_length=512, 
            padding=True, 
            truncation=True, 
            return_tensors='pt'
        ).to(DEVICE)
        
        with torch.no_grad():
            outputs = self.embed_model(**inputs)
            embedding = self.last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])
            embedding = F.normalize(embedding, p=2, dim=1)
            
        return embedding[0].cpu().tolist()

    def retrieve_hybrid(self, query, top_k=30):
        """
        æ··åˆæ£€ç´¢ï¼šä» Vector å’Œ BM25 å„å– top_kï¼Œå–å¹¶é›†
        """
        candidates = {} # {chunk_id: chunk_data}

        # --- A. å‘é‡æ£€ç´¢ ---
        query_vec = self.get_query_embedding(query)
        vec_results = self.collection.query(
            query_embeddings=[query_vec],
            n_results=top_k
        )
        
        # å¤„ç† Vector ç»“æœ
        if vec_results['ids']:
            for i, doc_id in enumerate(vec_results['ids'][0]):
                candidates[doc_id] = {
                    'text': vec_results['documents'][0][i],
                    'metadata': vec_results['metadatas'][0][i],
                    'source': 'vector'
                }

        # --- B. BM25 æ£€ç´¢ ---
        tokenized_query = query.lower().split() # ç®€å•åˆ†è¯ï¼Œä¹Ÿå¯å¤ç”¨ nltk
        bm25_top_n = self.bm25.get_top_n(tokenized_query, self.bm25_chunks, n=top_k)
        
        # å¤„ç† BM25 ç»“æœ
        for chunk in bm25_top_n:
            doc_id = chunk['id']
            if doc_id not in candidates:
                candidates[doc_id] = {
                    'text': chunk['text'],
                    'metadata': chunk['metadata'],
                    'source': 'bm25'
                }
            else:
                candidates[doc_id]['source'] = 'hybrid' # ä¸¤è¾¹éƒ½æ‰¾åˆ°äº†

        return list(candidates.values())

    def rerank(self, query, candidates, top_n=5):
        """
        ä½¿ç”¨ Cross-Encoder å¯¹å€™é€‰é›†è¿›è¡Œé‡æ’åº
        """
        if not candidates:
            return []

        # æ„å»º pairs: [[query, doc1], [query, doc2], ...]
        pairs = [[query, doc['text']] for doc in candidates]
        
        with torch.no_grad():
            inputs = self.rerank_tokenizer(
                pairs, 
                padding=True, 
                truncation=True, 
                return_tensors='pt', 
                max_length=512
            ).to(DEVICE)
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            scores = self.rerank_model(**inputs, return_dict=True).logits.view(-1).float()
            
            # å½’ä¸€åŒ–åˆ†æ•° (Sigmoid)
            scores = torch.sigmoid(scores)

        # å°†åˆ†æ•°é™„åŠ åˆ° candidates
        ranked_results = []
        for i, score in enumerate(scores):
            candidates[i]['score'] = score.item()
            ranked_results.append(candidates[i])

        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        ranked_results.sort(key=lambda x: x['score'], reverse=True)
        
        return ranked_results[:top_n]

    def search(self, query):
        print(f"\nğŸ” Query: {query}")
        
        # 1. æ··åˆå¬å› (Recall) - è·å–å¤§é‡å€™é€‰ (æ¯”å¦‚ 60 ä¸ª)
        candidates = self.retrieve_hybrid(query, top_k=30)
        print(f"   - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (Vector + BM25)")
        
        # 2. é‡æ’åº (Rerank) - æç‚¼ Top 5
        final_results = self.rerank(query, candidates, top_n=5)
        
        # 3. å±•ç¤ºç»“æœ
        print(f"   - Rerank å®Œæˆï¼Œç²¾é€‰ Top {len(final_results)}:\n")
        for i, res in enumerate(final_results):
            score = res['score']
            source = res['source']
            title = res['metadata']['title']
            # æˆªå–éƒ¨åˆ†å†…å®¹å±•ç¤º
            content_preview = res['text'].split('\nContent:\n')[-1][:150].replace('\n', ' ')
            
            print(f"[{i+1}] Score: {score:.4f} | Source: {source} | Title: {title}")
            print(f"    {content_preview}...")
            print("-" * 50)

        return final_results

if __name__ == "__main__":
    # åˆå§‹åŒ–å¼•æ“
    rag = UltimateRAG()
    
    # --- æµ‹è¯•æ¡ˆä¾‹ ---
    
    # æµ‹è¯• 1: æ¨¡ç³Šè¯­ä¹‰ (æµ‹è¯• Embedding)
    rag.search("Which plant can slow down zombies?")
    
    # æµ‹è¯• 2: ç²¾ç¡®æ•°å€¼ (æµ‹è¯• BM25 + Infobox æå–)
    rag.search("What is the sun cost of Peashooter?")
    
    # æµ‹è¯• 3: æ¯”è¾ƒ/ç­–ç•¥ (æµ‹è¯• Rerank é€»è¾‘èƒ½åŠ›)
    rag.search("Difference between Cherry Bomb and Jalapeno")

    # å¦‚æœä½ æƒ³æ‰‹åŠ¨è¾“å…¥:
    while True:
        q = input("\nè¯·è¾“å…¥é—®é¢˜ (è¾“å…¥ q é€€å‡º): ")
        if q.lower() == 'q': break
        rag.search(q)