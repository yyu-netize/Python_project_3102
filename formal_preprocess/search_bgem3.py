import torch
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import chromadb
from rank_bm25 import BM25Okapi
import pickle
import json
import os
from nltk.tokenize import word_tokenize
import nltk
from openai import OpenAI

print(torch.__version__)
# é…ç½®
DB_DIR = "./chroma_db_m3"
BM25_PATH = "./bm25_m3.pkl"
 
# æ¨¡å‹å®šä¹‰
MODEL_NAME = "/home/zbz/models/bge-m3" # è¿™ä¸ªè·¯å¾„æŒ‡å‘é¢„è®­ç»ƒçš„ SentenceTransformer æ¨¡å‹
RERANKER_MODEL_NAME = "/home/zbz/models/bge-reranker-v2-m3" # å¼ºåŠ›é‡æ’æ¨¡å‹
 
# æ˜¾å¡é…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- CONFIGURATION ---
SILICONFLOW_API_KEY = "sk-bpdybiceumehnyfudsglnizvhqssgpsjpusvienlfgchijdl"  # SiliconFlow API å¯†é’¥
LLM_NAME = "Qwen/Qwen3-8B"      

# Initialize the Client pointing to SiliconFlow
client_llm = OpenAI(
    base_url="https://api.siliconflow.cn/v1/",  
    api_key=SILICONFLOW_API_KEY,
)
 
# --- è¯å…¸æ£€æŸ¥ ---
# ç¡®ä¿ nltk çš„ punkt è¯å…¸å¯ç”¨
for pkg in ["punkt", "punkt_tab"]:
    try:
        nltk.data.find(f"tokenizers/{pkg}")
    except LookupError:
        nltk.download(pkg)
 
class UltimateRAG:
    def __init__(self):
        print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– RAG å¼•æ“...")

        self.client_llm = client_llm
        self.llm_name = LLM_NAME

        # 1. åŠ è½½ Embedding æ¨¡å‹ (SentenceTransformer)
        print(f" [1/4] åŠ è½½ Embedding æ¨¡å‹: {MODEL_NAME} ...")
        self.model = SentenceTransformer(MODEL_NAME, device=DEVICE)
        self.model.max_seq_length = 1024 # è®¾ç½®æœ€å¤§è¾“å…¥é•¿åº¦
    
        # 2. åŠ è½½ Reranker æ¨¡å‹ (Cross-Encoder)
        print(f" [2/4] åŠ è½½ Reranker æ¨¡å‹: {RERANKER_MODEL_NAME} ...")
        self.rerank_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)
        self.rerank_model = AutoModelForSequenceClassification.from_pretrained(
            RERANKER_MODEL_NAME,
            torch_dtype=torch.float16
            ).to(DEVICE)
        self.rerank_model.eval()
    
        # 2. è¿æ¥ ChromaDB
        print(" [3/4] è¿æ¥å‘é‡æ•°æ®åº“...")
        self.client = chromadb.PersistentClient(path=DB_DIR)
        self.collection = self.client.get_collection("pvz_knowledge_m3")
    
        # 3. åŠ è½½ BM25
        print(" [4/4] åŠ è½½ BM25 ç´¢å¼•...")
        with open(BM25_PATH, 'rb') as f:
            data = pickle.load(f)
            self.bm25 = data['bm25']
            self.bm25_chunks = data['chunks'] # BM25 éœ€è¦åŸå§‹ chunks åˆ—è¡¨æ¥å®šä½ç»“æœ
    
        print("âœ… RAG å¼•æ“å°±ç»ª! ç­‰å¾…æŒ‡ä»¤...\n")
 
 
    def get_query_embedding(self, query):
        """
        ä½¿ç”¨ SentenceTransformer è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        """
        # æŒ‡ä»¤ï¼šå®šä¹‰ä»»åŠ¡æ€§è´¨
        task_instruction = "Retrieve detailed attributes, stats, and strategies for Plants vs. Zombies game entities."
        # æ ¼å¼ï¼šInstruction + \n + Query
        prompt = f"Instruction: {task_instruction}\nQuery: {query}"
        embedding = self.model.encode(prompt, convert_to_tensor=True, normalize_embeddings=True)
        return embedding.cpu().tolist()
    
    def hyde_generate_doc(self, query):
        """
        ç”Ÿæˆ HyDE è™šæ„æ–‡æ¡£ï¼ˆHypothetical Answerï¼‰
        """
        prompt = f"""
You are a knowledgeable assistant. 
Directly generate a factual, detailed document that would answer the following question.
Do NOT mention that this is a hypothetical document.
Ensure the hypothetical answer is written as complete sentences and does not end abruptly.
---
Question: {query}
---
Hypothetical Document:
"""

        try:
            response = self.client_llm.chat.completions.create(
                model=self.llm_name,
                messages=[
                        {"role": "user", "content": prompt},
                    ],
                    stop=None,
                    temperature=0.7,
                    top_p=1.0,
                    n=1,
            )
            doc = response.choices[0].message.content.strip()
            return doc

        except Exception as e:
            print(f"HyDE generation error: {e}")
            return ""

    
    def retrieve_bm25(self, query, top_k=30):
        tokenized_query = query.lower().split()
        bm25_top_n = self.bm25.get_top_n(tokenized_query, self.bm25_chunks, n=top_k)
        results = []
        for chunk in bm25_top_n:
            results.append({
                'text': chunk['text'],
                'metadata': chunk['metadata'],
                'source': 'bm25'
            })
        return results

    def retrieve_dense(self, query, top_k=30):
        """
        model: SentenceTransformer å¯¹è±¡
        collection: ChromaDB collection
        """
        query_vec = self.get_query_embedding(query)
        vec_results = self.collection.query(query_embeddings=[query_vec], n_results=top_k)
        results = []
        if vec_results['ids']:
            for i, doc_id in enumerate(vec_results['ids'][0]):
                results.append({
                    'text': vec_results['documents'][0][i],
                    'metadata': vec_results['metadatas'][0][i],
                    'source': 'dense'
                })
        return results

 
    def retrieve_hybrid(self, query, top_k=30):
        """
        æ··åˆæ£€ç´¢ï¼šä» Vector å’Œ BM25 å„å– top_kï¼Œå–å¹¶é›†
        """
        candidates = {} # {chunk_id: chunk_data}
        
        # --- A. å‘é‡æ£€ç´¢ ---
        query_vec = self.get_query_embedding(query)
        vec_results = self.collection.query(
            query_embeddings=[query_vec],
            n_results=top_k
        )
        
        # å¤„ç† Vector ç»“æœ
        if vec_results['ids']:
            for i, doc_id in enumerate(vec_results['ids'][0]):
                candidates[doc_id] = {
                    'text': vec_results['documents'][0][i],
                    'metadata': vec_results['metadatas'][0][i],
                    'source': 'vector'
                }
        
        # --- B. BM25 æ£€ç´¢ ---
        tokenized_query = query.lower().split() # ç®€å•åˆ†è¯
        bm25_top_n = self.bm25.get_top_n(tokenized_query, self.bm25_chunks, n=top_k)
        
        # å¤„ç† BM25 ç»“æœ
        for chunk in bm25_top_n:
            doc_id = chunk['id']
            if doc_id not in candidates:
                candidates[doc_id] = {
                    'text': chunk['text'],
                    'metadata': chunk['metadata'],
                    'source': 'bm25'
                }
            else:
                candidates[doc_id]['source'] = 'hybrid' # ä¸¤è¾¹éƒ½æ‰¾åˆ°äº†
        
        return list(candidates.values())
 
 
    def rerank(self, query, candidates, top_n=5):
        """
        ä½¿ç”¨ Cross-Encoder å¯¹å€™é€‰é›†è¿›è¡Œé‡æ’åº
        """
        if not candidates:
            return []
        
        # æ„å»º pairs: [[query, doc1], [query, doc2], ...]
        pairs = [[query, doc['text']] for doc in candidates]
        
        with torch.no_grad():
            inputs = self.rerank_tokenizer(
                pairs, 
                padding=True, 
                truncation=True, 
                return_tensors='pt', 
                max_length=512
            ).to(DEVICE)
        
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            scores = self.rerank_model(**inputs, return_dict=True).logits.view(-1).float()
            
            # å½’ä¸€åŒ–åˆ†æ•° (Sigmoid)
            scores = torch.sigmoid(scores)
        
        # å°†åˆ†æ•°é™„åŠ åˆ° candidates
        ranked_results = []
        for i, score in enumerate(scores):
            candidates[i]['score'] = score.item()
            ranked_results.append(candidates[i])
        
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        ranked_results.sort(key=lambda x: x['score'], reverse=True)
        
        return ranked_results[:top_n]
 
    def search(self, query, retrieve_mode):
        print(f"\nğŸ” Query: {query}")
        
        candidates = []

        if retrieve_mode == "hyde":
            hyde_doc = self.hyde_generate_doc(query)
            print(f"\nğŸ“„ HyDE Generated Document:\n{hyde_doc}\n")

            # dense embedding from HyDE doc
            candidates = self.retrieve_dense(hyde_doc, top_k=30)
            print(f"   - HyDE Dense Retrieval æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ")

        if (retrieve_mode == "hybrid"):
            # 1. æ··åˆå¬å› (Recall) - è·å–å¤§é‡å€™é€‰ (æ¯”å¦‚ 30 ä¸ª)
            candidates = self.retrieve_hybrid(query, top_k=30)
            print(f" - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (Vector + BM25)")
            
        if (retrieve_mode == "dense"):
            # 1. æ··åˆå¬å› (Recall) - è·å–å¤§é‡å€™é€‰ (æ¯”å¦‚ 30 ä¸ª)
            candidates = self.retrieve_dense(query, top_k=30)
            print(f" - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (Vector)")
            
        if (retrieve_mode == "bm25"):
            # 1. æ··åˆå¬å› (Recall) - è·å–å¤§é‡å€™é€‰ (æ¯”å¦‚ 30 ä¸ª)
            candidates = self.retrieve_bm25(query, top_k=30)
            print(f" - å¬å›é˜¶æ®µæ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ (BM25)")
        
        # 2. é‡æ’åº (Rerank) - æç‚¼ Top 5
        final_results = self.rerank(query, candidates, top_n=5)
            
        # 3. å±•ç¤ºç»“æœ
        print(f" - Rerank å®Œæˆï¼Œç²¾é€‰ Top {len(final_results)}:\n")
        for i, res in enumerate(final_results):
            score = res['score'] if 'score' in res else 0 # é»˜è®¤åˆ†æ•°ä¸º0
            source = res['source']
            title = res['metadata']['title']
            # æˆªå–éƒ¨åˆ†å†…å®¹å±•ç¤º
            content_preview = res['text'].split('\nContent:\n')[-1][:150].replace('\n', ' ')
                
            print(f"[{i+1}] Score: {score:.4f} | Source: {source} | Title: {title}")
            print(f" {content_preview}...")
            print("-" * 50)
            
        return final_results
            
 
if __name__ == "__main__":
    # åˆå§‹åŒ–å¼•æ“
    rag = UltimateRAG()
    
    # æµ‹è¯•æ¡ˆä¾‹
    rag.search("Which plant can slow down zombies?", retrieve_mode="hybrid")
    rag.search("What is the sun cost of Peashooter?", retrieve_mode="dense")
    rag.search("Difference between Cherry Bomb and Jalapeno", retrieve_mode="hyde")
    
    # å¦‚æœä½ æƒ³æ‰‹åŠ¨è¾“å…¥:
    while True:
        q = input("\nè¯·è¾“å…¥é—®é¢˜ (è¾“å…¥ q é€€å‡º): ")
        if q.lower() == 'q': break
        rag.search(q, retrieve_mode="hybrid")